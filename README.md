# LLM-Test-Harness: A Framework for Empirical Analysis of Test Generation

## Overview
This project provides a flexible and extensible framework for conducting empirical studies on automated test generation. The primary goal is to systematically evaluate and compare the effectiveness, cost, and maintainability of tests generated by Large Language Models (LLMs) versus traditional automated test generation tools.

While recent research has shown the potential of LLMs for generating unit tests, their application to more complex integration testing remains a significant research gap. This framework is designed to address this by providing the tools to analyze LLM performance on multi-component interaction scenarios, with a novel focus on measuring test maintainability through automated static analysis.

The core architecture is plug-and-play, allowing researchers to easily add, remove, or swap different test generators (both LLMs and traditional tools) to conduct a wide range of comparative studies.

## Research Focus
This framework is designed to investigate the following core research questions:
1. RQ1 (Effectiveness): Which approach—advanced LLM prompting or a traditional automated tool—generates more effective integration tests in terms of bug-finding ability and code coverage?
2. RQ2 (Maintainability): How does the maintainability of LLM-generated integration tests compare to those from traditional tools, an aspect largely unexplored in existing research?
3. RQ3 (Cost): What are the associated resource costs (e.g., generation time, computational expense, required human effort) for creating integration tests with each approach?
4. RQ4 (Trade-off): What is the overall cost-benefit trade-off, and do the quality improvements (especially in maintainability) from advanced LLM prompting justify their higher costs for integration testing?